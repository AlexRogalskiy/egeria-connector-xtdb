{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "outside-pakistan",
   "metadata": {},
   "source": [
    "# Conformance Test Suite results\n",
    "\n",
    "This notebook has been used to analyze and compare the results from various Conformance Test Suite runs, primarily to investigate the general (mean) response times of various methods implemented by the repository via the MetadataCollection interfaces.\n",
    "\n",
    "## Mean execution times\n",
    "\n",
    "Following calculates the mean execution times, per method, across 3 different runs of the repository connector at the same scale factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_locations = []\n",
    "results_locations.append(\"2.7-SNAPSHOT_21.02-1.15.0-beta/2/a\")\n",
    "results_locations.append(\"2.7-SNAPSHOT_21.02-1.15.0-beta/2/b\")\n",
    "results_locations.append(\"2.7-SNAPSHOT_21.02-1.15.0-beta/2/c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will simply print out and validate that the location defined exists\n",
    "import os\n",
    "\n",
    "def validateProfileResultsLocation(location):\n",
    "    profile_details_location = location + os.path.sep + \"profile-details\"\n",
    "    if os.path.isdir(profile_details_location):\n",
    "        print(\"Location exists:\", profile_details_location)\n",
    "    else:\n",
    "        print(\"ERROR: Location does NOT exist:\", profile_details_location)\n",
    "\n",
    "for location in results_locations:\n",
    "    validateProfileResultsLocation(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-bristol",
   "metadata": {},
   "source": [
    "Now that we have our locations defined from which to calculate overall statistics, we will define some functions that parse through the profile details of each CTS result for us and build up a data frame with all of the results of interest (methods, elapsed times, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Given a profileResult.requirementResults object, parse all of its positiveTestEvidence\n",
    "# and group the results by methodName\n",
    "def parseEvidence(df, repositoryName, requirementResults):\n",
    "    if (requirementResults is not None and 'positiveTestEvidence' in requirementResults):\n",
    "        print(\"Parsing evidence for:\", requirementResults['name'], \"(\" + repositoryName + \")\")\n",
    "        data_array = []\n",
    "        for evidence in requirementResults['positiveTestEvidence']:\n",
    "            if ('methodName' in evidence and 'elapsedTime' in evidence):\n",
    "                data = {\n",
    "                    'repo': repositoryName,\n",
    "                    'method_name': evidence['methodName'],\n",
    "                    'elapsed_time': evidence['elapsedTime'],\n",
    "                    'test_case_id': evidence['testCaseId'],\n",
    "                    'assertion_id': evidence['assertionId']\n",
    "                }\n",
    "                data_array.append(data)\n",
    "        df = df.append(pd.read_json(json.dumps(data_array), orient='records'), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Given a profile detail JSON file, retrieve all of its profileResult.requirementResults[] objects\n",
    "def parseRequirementResults(profileFile):\n",
    "    with open(profileFile) as f:\n",
    "        profile = json.load(f)\n",
    "    # This first case covers files retrieved via API\n",
    "    if ('profileResult' in profile and 'requirementResults' in profile['profileResult']):\n",
    "        return profile['profileResult']['requirementResults']\n",
    "    # This second case covers files created by the CLI client\n",
    "    elif ('requirementResults' in profile):\n",
    "        return profile['requirementResults']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Retrieve a listing of all of the profile detail JSON files\n",
    "def getAllProfiles(profileLocation):\n",
    "    detailsLocation = profileLocation + os.path.sep + \"profile-details\"\n",
    "    _, _, filenames = next(os.walk(detailsLocation))\n",
    "    full_filenames = []\n",
    "    for filename in filenames:\n",
    "        full_filenames.append(detailsLocation + os.path.sep + filename)\n",
    "    return full_filenames\n",
    "\n",
    "# Parse all of the provided profile file's details into the provided dataframe\n",
    "def parseProfileDetailsIntoDF(df, profileFile, qualifier):\n",
    "    profileResults = parseRequirementResults(profileFile)\n",
    "    if profileResults is not None:\n",
    "        for result in profileResults:\n",
    "            df = parseEvidence(df, qualifier, result)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll = pd.DataFrame({'repo': [], 'method_name': [], 'elapsed_time': [], 'test_case_id': [], 'assertion_id': []})\n",
    "for location in results_locations:\n",
    "    profile_files = getAllProfiles(location)\n",
    "    for profile_file in profile_files:\n",
    "        dfAll = parseProfileDetailsIntoDF(dfAll, profile_file, 'Crux')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-smith",
   "metadata": {},
   "source": [
    "Now that all of the details are in a data frame, we can use Python to quickly calculate various statistics on a method-by-method basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "dfAll.groupby('method_name').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-exhaust",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
